from pyspark.sql import SparkSession
import boto3
from io import BytesIO
import sys
from awsglue.utils import getResolvedOptions
import matplotlib.pyplot as plt
import json
from pyspark.sql.functions import col, sum as sum_
from pyspark.sql import functions as F
from pyspark.sql.functions import col, regexp_replace, isnan, when, count, countDistinct, mean, stddev
from pyspark.sql.types import StringType, DoubleType, IntegerType
import re
from pyspark.sql.functions import col, isnan, when, count, mean, stddev, approx_count_distinct


def read_target_variable_from_s3(bucket, key):
    s3_client = boto3.client('s3')
    obj = s3_client.get_object(Bucket=bucket, Key=key)
    target_variable = obj['Body'].read().decode('utf-8').strip()
    return target_variable


def clean_column_name(column_name):
    return re.sub(r"[ ,;{​​​​​​​}​​​​​​​()\n\t=]", "_", column_name)


def main():
    # Initialize Spark session
    spark = SparkSession.builder.appName("DataFrameAnalysis").getOrCreate()


    # Fetch arguments passed to the Glue job
    args = getResolvedOptions(sys.argv, ['INPUT_PATH', 'OUTPUT_BUCKET', 'OUTPUT_PATH'])
    input_path = args['INPUT_PATH']
    output_bucket = args['OUTPUT_BUCKET']
    output_path = args['OUTPUT_PATH']
    
       # Define the target variable file location in S3
    # bucket_name = 'mycloudcomputingproject'  # replace with your S3 bucket name
    # target_variable_key = 'target_variable.txt'  # consistent file name
    # # Read the target variable
    # target_variable = read_target_variable_from_s3(bucket_name, target_variable_key)


    # Read CSV file from S3
    df = spark.read.csv(f"s3://{​​​​​​​input_path}​​​​​​​", header=True, inferSchema=True)


    # Remove duplicates
    df = df.dropDuplicates()


    # Clean column names
    new_column_names = [clean_column_name(c) for c in df.columns]
    df = df.toDF(*new_column_names)


    # Descriptive statistics
    json_data = {​​​​​​​}​​​​​​​
    json_data['total_rows'] = df.count()
    column_stats = {​​​​​​​}​​​​​​​


    # Identifying categorical and quantitative columns
    categorical_columns = []
    quantitative_columns = []


    for column in df.columns:
        column_data = {​​​​​​​}​​​​​​​
        column_type = df.schema[column].dataType


        if isinstance(column_type, StringType):
            categorical_columns.append(column)
        elif isinstance(column_type, (DoubleType, IntegerType)):
            quantitative_columns.append(column)


        column_data['distinct_count'] = df.select(column).distinct().count()
        column_data['missing_values'] = df.filter((col(column).isNull()) | (isnan(col(column)))).count()


        if isinstance(column_type, (DoubleType, IntegerType)):
            column_data['mean'] = df.select(mean(col(column))).collect()[0][0]
            column_data['stddev'] = df.select(stddev(col(column))).collect()[0][0]


        column_stats[column] = column_data


    json_data['column_stats'] = column_stats
    json_data['categorical_columns'] = categorical_columns
    json_data['quantitative_columns'] = quantitative_columns


    
    # Handling missing values
    total_rows = df.count()
    if total_rows > 0:


        for column in df.columns:
            missing_count = df.filter((col(column).isNull()) | (isnan(col(column)))).count()
            if missing_count / total_rows <= 0.3:
                # If missing values are less than 30%, drop rows
                df = df.filter(col(column).isNotNull())
            else:
                # If more than 30%, impute with median/mode
                if column in quantitative_columns:
                    median_value = df.approxQuantile(column, [0.5], 0.05)[0]
                    df = df.na.fill({​​​​​​​column: median_value}​​​​​​​)
                elif column in categorical_columns:
                    mode_value = df.groupBy(column).count().orderBy('count', ascending=False).first()[0]
                    df = df.na.fill({​​​​​​​column: mode_value}​​​​​​​)


        # Update total rows after handling missing values
        json_data['rows_after_handling_missing'] = df.count()
    else: pass


    # Save JSON result to S3
    s3_client = boto3.client('s3')
    json_key = output_path + input_path.split('/')[-1] + '_summary.json'
    s3_client.put_object(Bucket=output_bucket, Key=json_key, Body=json.dumps(json_data, indent=4))


    # Stop the Spark session
    spark.stop()


if __name__ == "__main__":
    main()
 
